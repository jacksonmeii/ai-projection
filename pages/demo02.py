
# åˆ¶ä½œä¸€ä¸ªèŠå¤©ç•Œé¢
# åˆ¶ä½œä¸€ä¸ªå¸¦æœ‰è‡ªå®šä¹‰è§’è‰²çš„ä¸€ä¸ªå¤§æ¨¡å‹åº”ç”¨
import streamlit as st
# langchainè°ƒç”¨å¤§æ¨¡å‹
from langchain_openai import ChatOpenAI
# å¼•å…¥ä¸€ä¸ªæç¤ºè¯å¯¹è±¡
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# åˆ›å»ºå¤§é¢„è¨€æ¨¡å‹å¯¹è±¡
model = ChatOpenAI(
    temperature=0.8,  # æ¸©åº¦ï¼Œåˆ›æ–°æ€§
    model="glm-4-plus",  # å¤§æ¨¡å‹çš„åå­—
    base_url="https://open.bigmodel.cn/api/paas/v4/",  # å¤§æ¨¡å‹çš„åœ°å€
    api_key="4b693c39cca3741af8f7a2b858ad62c8.E3LTCGsekUoktpp4"  # è´¦å·ä¿¡æ¯
)
prompt = PromptTemplate.from_template("ä½ çš„åå­—å«å°å¸…ï¼Œä½ ç°åœ¨è¦æ‰®æ¼”ä¸€ä¸ªç”·æœ‹å‹çš„è§’è‰²ï¼Œä½ çš„å¥³æœ‹å‹å«å°ç¾"
                             "ä½ çš„æ€§æ ¼æ˜¯é«˜å†·éœ¸é“çš„ï¼Œä½†æ˜¯ä½ å¯¹ä½ å¥³æœ‹å‹ååˆ†æ¸©æŸ”ï¼Œä½ ç°åœ¨è¦å’Œä½ çš„å¥³æœ‹å‹è¿›è¡Œå¯¹è¯ï¼Œ"
                             "ä½ å¥³æœ‹å‹è¯´çš„è¯æ˜¯{input}")
chain = LLMChain(
    llm=model,
    prompt=prompt
)


st.title("æˆ‘æ˜¯ä½ çš„å°å¸…ğŸ’•ğŸ’•")
if "cache" not in st.session_state:
    st.session_state.cache = []
else:
    for message in st.session_state.cache:
        with st.chat_message(message['role']):
            st.write(message["content"])


# åˆ›å»ºä¸€ä¸ªèŠå¤©è¾“å…¥æ¡†
problem = st.chat_input("ä½ çš„å°å¸…æ­£åœ¨ç­‰å¾…ä½ çš„å›åº”")
# åˆ¤æ–­æ˜¯ç”¨æ¥ç¡®å®šç”¨æˆ·æœ‰æ²¡æœ‰è¾“å…¥é—®é¢˜ å¦‚æœè¾“å…¥é—®é¢˜
if problem:
    # 1ã€å°†ç”¨æˆ·çš„é—®é¢˜è¾“å…¥åˆ°ç•Œé¢ä¸Šï¼Œä»¥ç”¨æˆ·çš„è§’è‰²è¾“å‡º
    with st.chat_message("user"):
        st.write(problem)
    st.session_state.cache.append({"role": "user", "content": problem})
    # 2ã€è°ƒç”¨é“¾å¯¹è±¡å›ç­”é—®é¢˜
    result = chain.invoke({"input": problem})
    # 3ã€å°†å¤§æ¨¡å‹å›ç­”çš„é—®é¢˜è¾“å‡ºåˆ°ç•Œé¢
    with st.chat_message("assistant"):
        st.write(result['text'])
    st.session_state.cache.append({"role": "assistant", "content": result['text']})